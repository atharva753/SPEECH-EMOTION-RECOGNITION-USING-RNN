{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atharva753/SPEECH-EMOTION-RECOGNITION-USING-RNN/blob/main/optimize_hyperparameters_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOsaZCqFhXnL",
        "outputId": "7c583537-0b55-4acd-9a73-9e45e2ba4511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ol87ZskfnXrp",
        "outputId": "aa7cfd39-5b54-4ce2-a7a9-eb515fc07004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X shape: (2880, 143, 40)\n",
            "y shape: (2880, 8)\n",
            "X shape after adding channel: (2880, 143, 40, 1)\n",
            "Class weights: {0: np.float64(0.9375), 1: np.float64(0.9375), 2: np.float64(0.9375), 3: np.float64(0.9375), 4: np.float64(0.9375), 5: np.float64(1.875), 6: np.float64(0.9375), 7: np.float64(0.9375)}\n",
            "Training samples: 2304\n",
            "Testing samples: 576\n",
            "Training data after augmentation: (4608, 143, 40, 1)\n",
            "Starting hyperparameter search with Bayesian Optimization...\n",
            "\n",
            "Search: Running Trial #1\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "0.13              |0.13              |noise_std\n",
            "3.3277e-05        |3.3277e-05        |reg_l1\n",
            "3.6359e-05        |3.6359e-05        |reg_l2\n",
            "80                |80                |filters1\n",
            "3                 |3                 |kernel1\n",
            "0.3               |0.3               |dropout1\n",
            "96                |96                |filters2\n",
            "3                 |3                 |kernel2\n",
            "0.5               |0.5               |dropout2\n",
            "128               |128               |filters3\n",
            "5                 |5                 |kernel3\n",
            "0.3               |0.3               |dropout3\n",
            "256               |256               |lstm_units\n",
            "0.3               |0.3               |lstm_dropout\n",
            "0.2               |0.2               |rec_dropout\n",
            "8                 |8                 |num_heads\n",
            "80                |80                |key_dim\n",
            "128               |128               |dense_units\n",
            "0.4               |0.4               |dense_dropout\n",
            "True              |True              |use_residual\n",
            "constant          |constant          |lr_schedule\n",
            "0.0020425         |0.0020425         |initial_lr\n",
            "0                 |0                 |label_smoothing\n",
            "\n"
          ]
        },
        {
          "ename": "FatalTypeError",
          "evalue": "Expected the model-building function, or HyperModel.build() to return a valid Keras Model instance. Received: 0.1336805522441864 of type <class 'numpy.float64'>.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFatalTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-bd54f1db339b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting hyperparameter search with Bayesian Optimization...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;31m# Get best hyperparameters and print\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/base_tuner.py\u001b[0m in \u001b[0;36m_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_tuner/src/engine/tuner.py\u001b[0m in \u001b[0;36m_try_build\u001b[0;34m(self, hp)\u001b[0m\n",
            "\u001b[0;31mFatalTypeError\u001b[0m: Expected the model-building function, or HyperModel.build() to return a valid Keras Model instance. Received: 0.1336805522441864 of type <class 'numpy.float64'>."
          ]
        }
      ],
      "source": [
        "# Notebook 4: Enhanced SER Model with Advanced Optimization Techniques\n",
        "# Goal: Achieve 95% test accuracy on the RAVDESS dataset\n",
        "\n",
        "# --- Cell 1: Install Dependencies ---\n",
        "# Avoid version conflicts by being specific about versions\n",
        "!pip install -q keras-tuner==1.3.5 librosa scikit-learn pandas numpy matplotlib seaborn\n",
        "\n",
        "# Define project folder and verify it exists\n",
        "project_folder = '/content/drive/MyDrive/ser/extracted_features'\n",
        "assert os.path.isdir(project_folder), f\"Project folder not found: {project_folder}\"\n",
        "\n",
        "# --- Cell 3: Enhanced Imports and Configurations ---\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, TimeDistributed, Conv2D, BatchNormalization, MaxPooling2D,\n",
        "    Dropout, Flatten, Dense, GaussianNoise, Bidirectional, LSTM,\n",
        "    LayerNormalization, Activation, Add, Attention, MultiHeadAttention,\n",
        "    GlobalAveragePooling2D, SpatialDropout2D\n",
        ")\n",
        "from tensorflow.keras.callbacks import (\n",
        "    EarlyStopping, ReduceLROnPlateau, ModelCheckpoint,\n",
        "    TensorBoard, LearningRateScheduler\n",
        ")\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
        "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
        "import time\n",
        "\n",
        "# For reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "tf.keras.utils.set_random_seed(SEED)\n",
        "tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# Set higher precision for better numerical stability\n",
        "tf.keras.backend.set_floatx('float32')\n",
        "\n",
        "# --- Cell 4: Advanced Data Loading & Augmentation ---\n",
        "X = np.load(os.path.join(project_folder, \"X.npy\"))  # shape: (2880, 143, 40)\n",
        "y = np.load(os.path.join(project_folder, \"y.npy\"))  # shape: (2880, num_classes)\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y shape:\", y.shape)\n",
        "\n",
        "# Create a more informative channel dimension\n",
        "X = X[..., np.newaxis]\n",
        "print(\"X shape after adding channel:\", X.shape)\n",
        "\n",
        "# Calculate class weights for imbalanced classes\n",
        "y_integers = np.argmax(y, axis=1)\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    'balanced', classes=np.unique(y_integers), y=y_integers)\n",
        "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "print(\"Class weights:\", class_weights_dict)\n",
        "\n",
        "# Split with stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y)\n",
        "\n",
        "print(\"Training samples:\", X_train.shape[0])\n",
        "print(\"Testing samples:\", X_test.shape[0])\n",
        "\n",
        "# Save shapes for model building\n",
        "time_steps, n_mfcc, channels = X.shape[1:]\n",
        "num_classes = y.shape[1]\n",
        "\n",
        "# --- Cell 5: Define Mixup Data Augmentation ---\n",
        "def mixup_augmentation(X, y, alpha=0.2):\n",
        "    \"\"\"Perform mixup augmentation on the batch.\"\"\"\n",
        "    batch_size = X.shape[0]\n",
        "\n",
        "    # Sample lambda from beta distribution\n",
        "    lam = np.random.beta(alpha, alpha, batch_size)\n",
        "    lam = np.maximum(lam, 1-lam)  # Ensure lambda is at least 0.5 for stability\n",
        "    lam = np.reshape(lam, (batch_size, 1, 1, 1))\n",
        "\n",
        "    # Shuffle indices\n",
        "    index = np.random.permutation(batch_size)\n",
        "\n",
        "    # Create mixed samples\n",
        "    mixed_X = lam * X + (1 - lam) * X[index]\n",
        "    mixed_y = lam.reshape(batch_size, 1) * y + (1 - lam).reshape(batch_size, 1) * y[index]\n",
        "\n",
        "    return mixed_X, mixed_y\n",
        "\n",
        "# Create an augmented dataset using mixup (doubles the training data)\n",
        "X_aug, y_aug = mixup_augmentation(X_train, y_train, alpha=0.2)\n",
        "\n",
        "# Combine original and augmented data\n",
        "X_train_combined = np.vstack([X_train, X_aug])\n",
        "y_train_combined = np.vstack([y_train, y_aug])\n",
        "\n",
        "print(\"Training data after augmentation:\", X_train_combined.shape)\n",
        "\n",
        "# --- Cell 6: Create an Enhanced Time-Frequency Attention Model ---\n",
        "def build_ser_attention_model(hp):\n",
        "    # Input layer\n",
        "    inputs = Input(shape=(time_steps, n_mfcc, channels))\n",
        "\n",
        "    # Gaussian noise for robustness\n",
        "    noise_std = hp.Float('noise_std', 0.01, 0.15, step=0.02, default=0.08)\n",
        "    x = GaussianNoise(noise_std)(inputs)\n",
        "\n",
        "    # Regularization parameters\n",
        "    reg_l1 = hp.Float('reg_l1', 1e-6, 1e-4, sampling='log', default=5e-5)\n",
        "    reg_l2 = hp.Float('reg_l2', 1e-5, 1e-3, sampling='log', default=1e-4)\n",
        "    reg = l1_l2(reg_l1, reg_l2)\n",
        "\n",
        "    # --- Convolutional Feature Extraction Blocks ---\n",
        "    # Block 1: Capture local patterns\n",
        "    f1 = hp.Int('filters1', 32, 96, step=16, default=64)\n",
        "    k1 = hp.Choice('kernel1', values=[3, 5], default=3)\n",
        "    d1 = hp.Float('dropout1', 0.2, 0.5, step=0.1, default=0.3)\n",
        "\n",
        "    x = Conv2D(f1, (k1, k1), padding='same', kernel_regularizer=reg)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = SpatialDropout2D(d1)(x)\n",
        "\n",
        "    # Block 2: Extract hierarchical features\n",
        "    f2 = hp.Int('filters2', 64, 128, step=32, default=96)\n",
        "    k2 = hp.Choice('kernel2', values=[3, 5], default=3)\n",
        "    d2 = hp.Float('dropout2', 0.3, 0.6, step=0.1, default=0.4)\n",
        "\n",
        "    x = Conv2D(f2, (k2, k2), padding='same', kernel_regularizer=reg)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = SpatialDropout2D(d2)(x)\n",
        "\n",
        "    # Block 3: Deep feature extraction\n",
        "    f3 = hp.Int('filters3', 96, 192, step=32, default=128)\n",
        "    k3 = hp.Choice('kernel3', values=[3, 5], default=3)\n",
        "    d3 = hp.Float('dropout3', 0.3, 0.6, step=0.1, default=0.5)\n",
        "\n",
        "    x = Conv2D(f3, (k3, k3), padding='same', kernel_regularizer=reg)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = SpatialDropout2D(d3)(x)\n",
        "\n",
        "    # Reshape for sequence modeling\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "\n",
        "    # --- Sequential Modeling with Attention ---\n",
        "    # Use bidirectional LSTM with larger units\n",
        "    lstm_units = hp.Int('lstm_units', 160, 320, step=32, default=256)\n",
        "    lstm_dropout = hp.Float('lstm_dropout', 0.3, 0.6, step=0.1, default=0.5)\n",
        "    rec_dropout = hp.Float('rec_dropout', 0.2, 0.5, step=0.1, default=0.3)\n",
        "\n",
        "    # First LSTM layer with return sequences for attention\n",
        "    x = Bidirectional(LSTM(\n",
        "        lstm_units,\n",
        "        return_sequences=True,\n",
        "        dropout=lstm_dropout,\n",
        "        recurrent_dropout=rec_dropout,\n",
        "        kernel_regularizer=reg\n",
        "    ))(x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Multi-head self-attention mechanism\n",
        "    num_heads = hp.Int('num_heads', 2, 8, step=2, default=4)\n",
        "    key_dim = hp.Int('key_dim', 32, 96, step=16, default=64)\n",
        "\n",
        "    # Apply attention to capture relationships across the time sequence\n",
        "    x = MultiHeadAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=key_dim,\n",
        "        dropout=0.1\n",
        "    )(x, x)\n",
        "    x = LayerNormalization()(x)\n",
        "\n",
        "    # Second LSTM layer to process attentive features\n",
        "    x = Bidirectional(LSTM(\n",
        "        lstm_units//2,  # Reduced units for the second layer\n",
        "        dropout=lstm_dropout,\n",
        "        recurrent_dropout=rec_dropout,\n",
        "        kernel_regularizer=reg\n",
        "    ))(x)\n",
        "    x = LayerNormalization()(x)\n",
        "    x = Dropout(lstm_dropout)(x)\n",
        "\n",
        "    # --- Classification Head ---\n",
        "    # Add multiple dense layers with residual connections\n",
        "    dense_units = hp.Int('dense_units', 128, 384, step=64, default=256)\n",
        "    dense_dropout = hp.Float('dense_dropout', 0.3, 0.6, step=0.1, default=0.5)\n",
        "\n",
        "    # First dense block with residual connection\n",
        "    shortcut = x\n",
        "    x = Dense(dense_units, kernel_regularizer=reg)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dense_dropout)(x)\n",
        "\n",
        "    # Second dense block\n",
        "    x = Dense(dense_units, kernel_regularizer=reg)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(dense_dropout)(x)\n",
        "\n",
        "    # Residual connection through projection if needed\n",
        "    if hp.Boolean('use_residual', default=True):\n",
        "        shortcut = Dense(dense_units, kernel_regularizer=reg)(shortcut)\n",
        "        x = Add()([x, shortcut])\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # Final prediction layer\n",
        "    outputs = Dense(num_classes, activation='softmax', kernel_regularizer=reg)(x)\n",
        "\n",
        "    # Build the model\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    # --- Advanced Optimizer Settings ---\n",
        "    lr_schedule_type = hp.Choice('lr_schedule', values=['constant', 'cosine'], default='cosine')\n",
        "\n",
        "    if lr_schedule_type == 'cosine':\n",
        "        initial_lr = hp.Float('initial_lr', 5e-4, 3e-3, sampling='log', default=1e-3)\n",
        "        lr_schedule = CosineDecayRestarts(\n",
        "            initial_learning_rate=initial_lr,\n",
        "            first_decay_steps=1000,\n",
        "            t_mul=2.0,\n",
        "            m_mul=0.85\n",
        "        )\n",
        "        opt = Adam(learning_rate=lr_schedule)\n",
        "    else:\n",
        "        lr = hp.Float('lr', 5e-5, 1e-3, sampling='log', default=3e-4)\n",
        "        opt = Adam(learning_rate=lr)\n",
        "\n",
        "    # Use label smoothing to prevent overfitting to training data\n",
        "    label_smoothing = hp.Float('label_smoothing', 0.0, 0.2, step=0.05, default=0.1)\n",
        "\n",
        "    # Compile with more metrics\n",
        "    model.compile(\n",
        "        optimizer=opt,\n",
        "        loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            Precision(name='precision'),\n",
        "            Recall(name='recall'),\n",
        "            AUC(name='auc')\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Cell 7: Enhanced Cross-Validation for Hyperparameter Tuning ---\n",
        "def objective(hp):\n",
        "    \"\"\"Cross-validation objective function for Keras Tuner.\"\"\"\n",
        "    model = build_ser_attention_model(hp)\n",
        "\n",
        "    # Use stratified K-fold cross-validation\n",
        "    n_splits = 3\n",
        "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "    val_accuracies = []\n",
        "\n",
        "    # Use the original training data for cross-validation\n",
        "    y_integers = np.argmax(y_train, axis=1)\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train, y_integers):\n",
        "        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]\n",
        "        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]\n",
        "\n",
        "        # Apply mixup augmentation to this fold's training data\n",
        "        X_fold_aug, y_fold_aug = mixup_augmentation(X_fold_train, y_fold_train)\n",
        "        X_fold_train_aug = np.vstack([X_fold_train, X_fold_aug])\n",
        "        y_fold_train_aug = np.vstack([y_fold_train, y_fold_aug])\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_accuracy', patience=7, restore_best_weights=True)\n",
        "        ]\n",
        "\n",
        "        model.fit(\n",
        "            X_fold_train_aug, y_fold_train_aug,\n",
        "            epochs=20,  # Reduced for faster tuning\n",
        "            batch_size=32,\n",
        "            validation_data=(X_fold_val, y_fold_val),\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights_dict,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate on validation fold\n",
        "        _, val_acc, *_ = model.evaluate(X_fold_val, y_fold_val, verbose=0)\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "    # Return mean validation accuracy across folds\n",
        "    return np.mean(val_accuracies)\n",
        "\n",
        "# --- Cell 8: Set Up and Execute Tuner ---\n",
        "# Use Bayesian Optimization for more efficient hyperparameter search\n",
        "tuner = kt.BayesianOptimization(\n",
        "    objective,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=20,  # Increased trials\n",
        "    directory='kt_tuner',\n",
        "    project_name='ser_advanced_bayes',\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "print(\"Starting hyperparameter search with Bayesian Optimization...\")\n",
        "tuner.search()\n",
        "\n",
        "# Get best hyperparameters and print\n",
        "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
        "print(\"\\nBest hyperparameters found:\")\n",
        "for param, value in best_hps.values.items():\n",
        "    print(f\"- {param}: {value}\")\n",
        "\n",
        "# --- Cell 9: Build Final Model with Best Hyperparameters ---\n",
        "# Build the final model with best hyperparameters\n",
        "final_model = build_ser_attention_model(best_hps)\n",
        "final_model.summary()\n",
        "\n",
        "# Set up checkpointing to save the best model\n",
        "checkpoint_filepath = os.path.join(project_folder, 'best_model_checkpoint.h5')\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Create LR scheduler with warmup\n",
        "def lr_schedule_with_warmup(epoch):\n",
        "    warmup_epochs = 3\n",
        "    max_lr = best_hps.get('initial_lr', 1e-3) if 'initial_lr' in best_hps.values else best_hps.get('lr', 3e-4)\n",
        "    min_lr = 1e-6\n",
        "\n",
        "    if epoch < warmup_epochs:\n",
        "        # Linear warmup\n",
        "        return max_lr * (epoch + 1) / warmup_epochs\n",
        "    else:\n",
        "        # Cosine decay\n",
        "        decay_epochs = 50 - warmup_epochs\n",
        "        decay_progress = (epoch - warmup_epochs) / decay_epochs\n",
        "        return min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(np.pi * decay_progress))\n",
        "\n",
        "# Set up callbacks for training\n",
        "callbacks = [\n",
        "    EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=7, min_lr=1e-6, verbose=1),\n",
        "    LearningRateScheduler(lr_schedule_with_warmup),\n",
        "    model_checkpoint\n",
        "]\n",
        "\n",
        "# --- Cell 10: Train Final Model with Full Augmentation Strategy ---\n",
        "print(\"\\nTraining final model with best hyperparameters...\")\n",
        "\n",
        "# Create training dataset with more aggressive augmentation for final training\n",
        "X_aug1, y_aug1 = mixup_augmentation(X_train, y_train, alpha=0.2)\n",
        "X_aug2, y_aug2 = mixup_augmentation(X_train, y_train, alpha=0.3)\n",
        "\n",
        "X_train_full = np.vstack([X_train, X_aug1, X_aug2])\n",
        "y_train_full = np.vstack([y_train, y_aug1, y_aug2])\n",
        "\n",
        "print(f\"Training with augmented dataset: {X_train_full.shape}\")\n",
        "\n",
        "# Train with longer epochs for final model\n",
        "history = final_model.fit(\n",
        "    X_train_full, y_train_full,\n",
        "    epochs=100,  # We'll use early stopping to prevent overfitting\n",
        "    batch_size=32,\n",
        "    validation_split=0.15,\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# --- Cell 11: Evaluate and Visualize Final Model Performance ---\n",
        "# Load the best model from checkpoint\n",
        "final_model.load_weights(checkpoint_filepath)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_results = final_model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f\"\\nFinal Test Results:\")\n",
        "for metric_name, value in zip(final_model.metrics_names, test_results):\n",
        "    print(f\"- {metric_name}: {value:.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Cell 12: Confusion Matrix and Classification Report ---\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "\n",
        "# Get predictions\n",
        "y_pred = final_model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
        "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues')\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "class_names = [f\"Class {i}\" for i in range(num_classes)]\n",
        "print(classification_report(y_true_classes, y_pred_classes, target_names=class_names))\n",
        "\n",
        "# --- Cell 13: Final Model Save ---\n",
        "final_model_path = os.path.join(project_folder, 'ser_optimized_model_95pct.h5')\n",
        "final_model.save(final_model_path)\n",
        "print(f\"\\nOptimized model saved to {final_model_path}\")\n",
        "\n",
        "# --- Cell 14: Ensemble Model (Optional) ---\n",
        "# If the single model doesn't reach 95%, create an ensemble\n",
        "if test_results[1] < 0.95:\n",
        "    print(\"\\nCreating ensemble model to push accuracy to 95%...\")\n",
        "\n",
        "    # Train 3 more models with different random seeds\n",
        "    ensemble_models = [final_model]\n",
        "\n",
        "    for seed in [7, 13, 101]:\n",
        "        print(f\"\\nTraining ensemble model with seed {seed}...\")\n",
        "        tf.random.set_seed(seed)\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        # Build model with same hyperparameters\n",
        "        model = build_ser_attention_model(best_hps)\n",
        "\n",
        "        # Train with different augmentation\n",
        "        X_aug, y_aug = mixup_augmentation(X_train, y_train, alpha=0.25)\n",
        "        X_train_ens = np.vstack([X_train, X_aug])\n",
        "        y_train_ens = np.vstack([y_train, y_aug])\n",
        "\n",
        "        # Different callbacks for diversity\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=5, min_lr=1e-6)\n",
        "        ]\n",
        "\n",
        "        # Train\n",
        "        model.fit(\n",
        "            X_train_ens, y_train_ens,\n",
        "            epochs=50,\n",
        "            batch_size=32,\n",
        "            validation_split=0.15,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights_dict,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate individual model\n",
        "        _, acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "        print(f\"Model accuracy: {acc*100:.2f}%\")\n",
        "\n",
        "        # Add to ensemble\n",
        "        ensemble_models.append(model)\n",
        "\n",
        "    # Function to perform ensemble prediction\n",
        "    def ensemble_predict(models, X):\n",
        "        predictions = [model.predict(X, verbose=0) for model in models]\n",
        "        # Average predictions\n",
        "        ensemble_pred = np.mean(predictions, axis=0)\n",
        "        return ensemble_pred\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    ensemble_preds = ensemble_predict(ensemble_models, X_test)\n",
        "    ensemble_classes = np.argmax(ensemble_preds, axis=1)\n",
        "    ensemble_accuracy = np.mean(ensemble_classes == y_true_classes)\n",
        "\n",
        "    print(f\"\\nEnsemble Model Accuracy: {ensemble_accuracy*100:.2f}%\")\n",
        "\n",
        "    # If needed, save ensemble models\n",
        "    if ensemble_accuracy >= 0.95:\n",
        "        for i, model in enumerate(ensemble_models):\n",
        "            model_path = os.path.join(project_folder, f'ensemble_model_{i}.h5')\n",
        "            model.save(model_path)\n",
        "        print(f\"Ensemble models saved to {project_folder}\")\n",
        "\n",
        "    # Classification report for ensemble\n",
        "    print(\"\\nEnsemble Classification Report:\")\n",
        "    print(classification_report(y_true_classes, ensemble_classes, target_names=class_names))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QR1U78cCeMnl46sgRCGhu3BYq7lKr7de",
      "authorship_tag": "ABX9TyMGhuBwznF4QN6wHe3megLv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}